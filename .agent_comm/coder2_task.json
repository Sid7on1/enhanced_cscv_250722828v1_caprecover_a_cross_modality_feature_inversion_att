{
  "agent_id": "coder2",
  "task_id": "task_5",
  "files": [
    {
      "name": "main_model.py",
      "purpose": "Main computer vision model",
      "priority": "high"
    },
    {
      "name": "training.py",
      "purpose": "Training pipeline",
      "priority": "high"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CV_2507.22828v1_CapRecover_A_Cross_Modality_Feature_Inversion_Att",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.CV_2507.22828v1_CapRecover-A-Cross-Modality-Feature-Inversion-Att with content analysis. Detected project type: computer vision (confidence score: 9 matches).",
    "key_algorithms": [
      "Residual",
      "Prompt",
      "Caption",
      "Embedding",
      "Reduce",
      "Version",
      "Q-Former",
      "Large",
      "Ated",
      "Split"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CV_2507.22828v1_CapRecover-A-Cross-Modality-Feature-Inversion-Att.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nCapRecover : A Cross-Modality Feature Inversion Attack\nFramework on Vision Language Models\nKedong Xiu\u2217\nNew York University\nNew York, NY, USA\nkedongxiu@zju.edu.cnSaiqian Zhang\u2020\nNew York University\nNew York, NY, USA\nsai.zhang@nyu.edu\nAbstract\nAs Vision-Language Models (VLMs) become increasingly integrated\ninto user-facing applications, they are often deployed in split DNN\nconfigurations, where the visual encoder (e.g., ResNet or ViT) runs\non user-side devices and only intermediate features are transmitted\nto the cloud for downstream processing. While this setup reduces\ncommunication overhead, the intermediate data features containing\nsensitive information can also expose users to privacy risks. Prior\nwork has attempted to reconstruct images from these features to\ninfer semantics, but such approaches often produce blurry images\nthat obscure semantic details. In contrast, the potential to directly\nrecover high-level semantic content \u2014 such as image labels or\ncaptions \u2014 via a cross-modality inversion attack remains largely\nunexplored. To address this gap, we propose CapRecover , a general\ncross-modality feature inversion framework that directly decodes\nsemantic information from intermediate features without requiring\nimage reconstruction. Additionally, CapRecover can be used to\nreverse engineer traditional neural networks for computer vision\ntasks, such as ViT, ResNet, and others.\nWe evaluate CapRecover across multiple widely used datasets\nand victim models. Our results demonstrate that CapRecover can\naccurately recover both image labels and captions without recon-\nstructing a single pixel. Specifically, it achieves up to 92.71% Top-1\naccuracy on the CIFAR-10 dataset for label recovery, and generates\nfluent and relevant captions from ResNet50\u2019s intermediate features\non COCO2017 dataset, with ROUGE-L scores up to 0.52. Further-\nmore, an in-depth analysis of ResNet-based models reveals that\ndeeper convolutional layers encode significantly more semantic\ninformation, whereas shallow layers contribute minimally to se-\nmantic leakage. Furthermore, we propose a straightforward and\neffective protection approach that adds random noise to the in-\ntermediate image features at each middle layer and subsequently\nremoves the noise in the following layer. Our experiments indicate\nthat this approach effectively prevents information leakage without\nadditional training costs.\n\u2217Also with Zhejiang University. Xiu finished this work when he was a remote intern\nat New York University.\n\u2020Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM \u201925, October 27\u201331, 2025, Dublin, Ireland.\n\u00a92025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-2035-2/2025/10\nhttps://doi.org/10.1145/3746027.3755203CCS Concepts\n\u2022Security and privacy ;\u2022Computing methodologies \u2192Artifi-\ncial intelligence ;\nKeywords\nFeature Inversion Attack, Cross-Modality, Vision Language Models\nACM Reference Format:\nKedong Xiu and Saiqian Zhang. 2025. CapRecover : A Cross-Modality Fea-\nture Inversion Attack Framework on Vision Language Models. In Proceed-\nings of the 33rd ACM International Conference on Multimedia (MM \u201925),\nOctober 27\u201331, 2025, Dublin, Ireland. ACM, New York, NY, USA, 9 pages.\nhttps://doi.org/10.1145/3746027.3755203\n1 Introduction\nThe rapid advancement of Vision-Language Models (VLMs) has fun-\ndamentally reshaped the landscape of multimodal AI, positioning\nthese models as the cornerstone of modern user-facing assistants.\nUnlike traditional Large Language Models (LLMs), VLMs seamlessly\nintegrate image understanding with natural language processing,\nenabling comprehensive interpretations of real-world data. By har-\nnessing vast amounts of textual and visual information, VLMs have\nachieved impressive results in tasks such as image captioning (e.g.,\nGPT-4o [ 21]), text-to-image generation (e.g., Stable Diffusion [ 1]),\nand optical character recognition. The success of architectures like\nCLIP [ 22] and BLIP2 [ 12] underscores their potential to drive sig-\nnificant innovations in both research and practical applications.\nDespite these advances, VLMs are not without vulnerabilities.\nRecent research has predominantly focused on security threats such\nas prompt jailbreaking\u2014where attackers manipulate models to pro-\nduce harmful outputs\u2014and prompt-stealing attacks that extract\nsensitive user prompts from generated images [ 4,18,24,25]. How-\never, one critical dimension remains underexplored: the leakage\nof sensitive information through intermediate feature representa-\ntions. As illustrated in Figure 1, an adversary who gains access to\nintermediate image features from the victim model (for instance,\nfrom a local device) could reconstruct the original image caption,\npotentially exposing private user data. This issue is especially rele-\nvant in the split DNN computing paradigm [ 6,17,19,31], where\na large model is divided into multiple blocks tailored to the com-\nputational capabilities of edge devices. In this setup, user data is\ninitially processed on the edge device using the first layers of the\nmodel, and intermediate results are transmitted to a remote server\nfor processing by later layers. This data transfer poses a risk, as it\ncan be intercepted and exploited by attackers to reconstruct user\ninputs. Consequently, understanding and mitigating the leakage of\nintermediate image features is imperative for protecting user data\nand maintaining the integrity of VLM-driven services.arXiv:2507.22828v1  [cs.CV]  30 Jul 2025\n\n--- Page 2 ---\nMM \u201925, October 27\u201331, 2025, Dublin, Ireland. Xiu et al.\n\u201cDescribe this Image\u201dVisualModel\nText \nProjectionLLM\u201cTwo zebras drinking \nwater from a pond \nwhile a duck swims \nnearby, surrounded \nby grassy terrain.\u201d\nStealIntermediate \nFeaturesCAPRECOVER\nAttacker\u201cTwo zebras quench their \nthirst at a waterhole as a \nsmall duck paddles nearby, \nset against a grassy backdrop.\u201dPromptImage\nRecoverImage features\nText features\nFeature \nConcatenationEncode\nEncode\nAttack\nFigure 1: Illustration of the cross-modality feature inversion attack scenario. In the depicted attack scenario, the adversary steals\nthe intermediate image features from the visual model. Leveraging these stolen features, the adversary employs CapRecover\nto reconstruct the image caption/label, potentially revealing sensitive or private information.\nPrior work [ 7,28,32] has explored reconstructing images from in-\ntermediate features to further infer their semantic content. However,\nthese methods are indirect and often suffer from preserving fine-\ngrained semantics with poor visual fidelity (e.g., blurriness, missing\ntextures), which could consequently limit their performance. More-\nover, some attackers may primarily focus on the semantic meaning\nin the target image, e.g., what is happening and who is involved.\nThis raises a critical yet underexplored question: Is it possible\u2014and\npotentially more effective\u2014for an attacker to directly recover high-\nlevel semantic information, such as image labels or captions, from\nintermediate features, without reconstructing the image at all? This\nnew form of feature inversion attack shifts the adversary\u2019s focus\nfrom pixel-level recovery to semantic reconstruction, and more\ndirectly threatens user privacy in practical scenarios.\n1.1 Our work and contributions\nIn this paper, we take a fundamentally different approach: instead\nof reconstructing the image, we directly recover/reconstruct the im-\nage\u2019s semantic content from the leaked intermediate image features.\nWe introduce CapRecover , a generic cross-modality feature in-\nversion framework that exposes a critical vulnerability in VLMs:\nthe capacity to reconstruct textual descriptions from intermediate\nimage features. CapRecover bridges intermediate visual features\nwith a pre-trained language model, bypassing image reconstruction\nentirely. By learning a lightweight projection layer between the\nvision and language domains, CapRecover enables accurate and\nfluent semantic recovery from commonly used encoders such as\nResNet and ViT.\nTo understand the privacy implications of this attack, we con-\nsider a threat model where the attacker passively observes the\nintermediate visual features sent from a user\u2019s device to the cloud\nin a split-VLM pipeline. The attacker has no access to the origi-\nnal image or the language module of the VLM, and aims to infer\nsemantic content directly from the encoder output. While a full\ndescription is provided in Sec. 2, we note here that this threat model\naligns with realistic deployment scenarios in edge-cloud systems\nand highlights a previously underestimated attack surface.\nWe evaluate CapRecover on multiple datasets and VLM ar-\nchitectures across two key tasks: image classification and imagecaptioning. Our experiments show that CapRecover can recover\nlabels and captions with high fidelity\u2014even without reconstructing\na single pixel. We further analyze how semantic leakage varies\nacross encoder depths and propose a simple, training-free defense\nmechanism that reduces leakage via reversible noise injection.\nWe summarize our contributions as follows:\n\u2022A general adversarial framework. We propose CapRe-\ncover , the first generic cross-modality feature inversion\nframework that directly reconstructs semantic information\nfrom the leaked intermediate image features, without re-\nquiring pixel-level image reconstruction. By leveraging a\nfeature-to-text alignment mechanism, CapRecover effec-\ntively recovers image labels and captions even in the absence\nof explicit textual outputs.\n\u2022Extensive evaluation and analysis. We evaluate CapRe-\ncover on both image classification and image captioning\ntasks using widely adopted datasets and victim models. CapRe-\ncover achieves up to 92.71% Top-1 accuracy on CIFAR-10 for\nlabel recovery and a ROUGE-L score of 0.52 on COCO2017\nfor caption reconstruction.\n\u2022An effective protection approach. We propose a straight-\nforward yet effective protection approach: Add random noise\nto the output of each layer in the victim model and remove\nthis noise in the subsequent layer. Our approach only needs a\nsmall noise cost without any additional training cost, which\ncan effectively mitigate the risks of sensitive information\nleakage from the intermediate image features.\n2 Threat Model\nWe consider a cross-modality feature inversion attack scenario\nwhere an adversary aims to reconstruct/recover the semantic de-\nscription/label corresponding to a given image by exploiting the\nintermediate image features Fproduced by the victim visual encoder\nV\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc54\ud835\udc52 . We assume a reasonable deployment situation consistent\nwith practical deployment [ 6,17] where VLMs are deployed in\nuser-facing applications or on edge devices, which commonly keep\nraw images and final captions locally private, yet may expose in-\ntermediate features (e.g., when features are transmitted to a cloud\nservice or temporarily stored in device memory).\n\n--- Page 3 ---\nCapRecover : A Cross-Modality Feature Inversion Attack Framework on Vision Language Models MM \u201925, October 27\u201331, 2025, Dublin, Ireland.\n\u2460Feature Projection\nIntermediate \nFeaturesFeature Projection Module\u201cTwo zebras drinking \u2026\u201d\n\u2461Feature -Text AlignmentFeature -Text \nAlignment ModuleLanguage \nModel\n\u201cTwo zebras \nquench their \u2026\u201d\u2462Caption GenerationGround Truth Caption ( Training Only )\nGenerated Caption\nFrozen Module\nOptimized Module\nFigure 2: Overview of CapRecover .CapRecover mainly\nconsists of: (1) Feature projection module, (2) Feature-text\nalignment module, and (3) Caption generation module. We\nfreeze the language model and optimize other modules.\nTable 1: Datasets Used in this paper.\nDataset\u2020Training size Sample size Test size Sample size\nCOCO2017 118,287 30,000 5,000 5,000\nFlickr8K 6,000 6,000 1,000 1,000\nImageNet-1K 1,281,167 12,000 50,000 900\nCIFAR-10 50,000 50,000 10,000 10,000\nTinyImageNet 100,000 100,000 10,000 10,000\n\u2020We use CIFAR-10 and TinyImageNet datasets for image label recovery\nand COCO2017, Flickr8K and ImageNet-1K datasets for image caption\nreconstruction, respectively.\n2.1 Adversary\u2019s Capabilities and Access.\nWe assume the adversary can intercept or obtain the victim model\u2019s\nintermediate visual representations Fbutcan not directly access to\nthe original input image \ud835\udc3cor its corresponding semantic description\n(e.g., the ground truth caption \ud835\udc47\ud835\udc50\ud835\udc4e\ud835\udc5dand image label \ud835\udc66\ud835\udc50\ud835\udc59\ud835\udc60). This can\noccur in practical scenarios where:\n\u2022The adversary intercepts intermediate features transmitted from\nan edge device to a cloud server responsible for caption or label\ngeneration;\n\u2022A malicious insider or malware on the user\u2019s device extracts\nintermediate features from memory.\nWe assume that the attacker knows the architecture of the victim\u2019s\nvisual encoder (e.g., ResNet50, ViT), including the position of inter-\nmediate layers used for downstream tasks. The attacker may also\nleverage auxiliary resources such as publicly available pretrained\nmodels to assist in decoding the extracted features. This setup aligns\nwith a standard white-box orgray-box threat model.\n2.2 Adversary\u2019s Objective.\nThe adversary\u2019s goal is to exploit the intermediate image features\nFproduced by the victim visual encoder ( F=V\ud835\udc3c\ud835\udc5a\ud835\udc4e\ud835\udc54\ud835\udc52(\ud835\udc3c)) to recon-\nstruct high-level semantic information. In this paper, we mainly\nconsider two forms of semantic targets:\n\u2022Caption Reconstruction: The attacker trains a cross-modality\ninversion attack model A\ud835\udf03to generate a textual caption \ud835\udc47\u2032cap=\nA\ud835\udf03(F)that approximates the ground-truth caption \ud835\udc47cap;\n\u2022Label Recovery: The attacker trains A\ud835\udf03as a classifier to predict\nthe image label \ud835\udc66\u2032\ncls=A\ud835\udf03(F)matching the true label \ud835\udc66cls.\nFor caption reconstruction, the attacker minimizes a semantic\nloss between the generated and reference captions:\narg min\n\ud835\udf03L(\ud835\udc47\u2032\ncap,\ud835\udc47cap), (1)Table 2: Information of different victim models and their\nintermediate layers\u2019 shapes.\nVictim model Intermediate layer Output feature dimension\u2217\nCLIP ViT16 base 512\nCLIP ViT32 no-proj 768\nResNet50\n(ResNet101)base 1024 (512)\nlayer1 [256, 56, 56] \u21921024\nlayer2 [512, 28, 28] \u21921024\nlayer3 [1024, 14, 14] \u21921024\nlayer4 [2048, 7, 7] \u21921024\nMobileNetV2base 1000MobileNetV3\n\u2217We use a ResNet-based projection module to transform those interme-\ndiate features retaining spatial dimensions (e.g., [32, 112, 112]) into a\nunified vectorized feature space (i.e., R1024).\nwhereL(\u00b7,\u00b7)is a semantic loss function (e.g., based on token-level\nor embedding-level similarity; see Sec. 3 for details).\nFor label recovery, the objective reduces to a standard classifica-\ntion loss:\narg min\n\ud835\udf03Lcls(\ud835\udc66\u2032\ncls,\ud835\udc66cls). (2)\nA successful attack implies that even without accessing raw\npixels, the intermediate image features alone are sufficient to com-\npromise user privacy by revealing semantic-level information.\n3 Methodology of CapRecover\nIn this section, we introduce the overview of CapRecover . During\ntraining, CapRecover aligns these intermediate image features\nwith the corresponding ground truth captions/labels, effectively\nlearning the mapping between visual representations and textual\ndescriptions. During inference, CapRecover relies exclusively on\nthe intermediate features to generate the image caption. While\nwe describe our model primarily in the context of image caption\nreconstruction, the overall framework applies equally to image\nlabel recovery with only minor task-specific adaptations.\n3.1 Overview of CapRecover\nAs shown in Figure 2, CapRecover is composed of three primary\nmodules: (1) Feature Projection Module, (2) Feature-Text Alignment\nModule, and (3) Caption Generation Module.\n3.1.1 Feature Projection Module CapRecover maps the victim\nmodel\u2019s intermediate features into a dimensionally fixed (e.g., 1024)\nfeature space via a projection layer. For example, given an input\nimage\ud835\udc3c\ud835\udc56, the victim model\u2019s middle layers produce intermediate\nfeatures F\ud835\udc56. When these features are already in vector form, i.e.,\nF\ud835\udc56\u2208R\ud835\udc51, we can simply apply a simple linear projection for F\ud835\udc56,\nwhich is\nF\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57\n\ud835\udc56=W\ud835\udc5dF\ud835\udc56+b\ud835\udc5d,W\ud835\udc5d\u2208R\ud835\udc51\u2032\u00d7\ud835\udc51,b\ud835\udc5d\u2208R\ud835\udc51\u2032(3)\nwhere F\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57\n\ud835\udc56\u2208R\ud835\udc51\u2032is the projected feature. \ud835\udc51\u2032is the dimension of\nthe projected feature space. W\ud835\udc5dandb\ud835\udc5dare learnable parameters\nof the feature projection layer.\nIn cases where the intermediate outputs retain spatial dimensions\n(i.e.,F\ud835\udc56\u2208R\ud835\udc36\u00d7\ud835\udc3b\u00d7\ud835\udc4awith\ud835\udc36,\ud835\udc3band\ud835\udc4adenoting the channel, height,\nand width, respectively), CapRecover first employs a ResNet-based\n\n--- Page 4 ---\nMM \u201925, October 27\u201331, 2025, Dublin, Ireland. Xiu et al.\nprojection module \ud835\udc54(\u00b7)to convert these spatial features into a vec-\ntorized form. The transformation is then expressed as:\nF\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57\n\ud835\udc56=W\ud835\udc5d\u00b7\ud835\udc54(F\ud835\udc56)+b\ud835\udc5d, \ud835\udc54 :R\ud835\udc36\u00d7\ud835\udc3b\u00d7\ud835\udc4a\u2192R\ud835\udc51\u2032(4)\nwhere W\ud835\udc5dandb\ud835\udc5dare learnable parameters of \ud835\udc54(\u00b7). This additional\nprojection module ensures that CapRecover can consistently pro-\ncess intermediate features from various victim models and different\nnetwork layers by mapping them into a unified feature space.\n3.1.2 Feature-Text Alignment Module CapRecover employs an\nalignment module (for image features and captions) to establish\na semantic correspondence between the projected intermediate\nimage features and the ground truth caption. Specifically, while\ntraining our CapRecover , the alignment module first tokenizes\nand embeds the ground truth caption for each image, resulting in\na sequence of text embeddings T\ud835\udc56. Second, to fuse these textual\ncues with the visual information, CapRecover further employs a Q-\nFormer model that leverages \ud835\udc3etrainable query tokens Q\u2208R\ud835\udc3e\u00d7\ud835\udc51\u2032.\nThe Q-Former performs cross-modal attention by interacting\nwith the projected features F\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57\n\ud835\udc56and the text embedding T\ud835\udc56, pro-\nducing enriched embeddings Z\ud835\udc56that capture the alignment between\nvisual and textual modalities, i.e.,\nZ\ud835\udc56=Q-Former(Q,F\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57\n\ud835\udc56,T\ud835\udc56), (5)\nwhere Z\ud835\udc56\u2208R\ud835\udc3e\u00d7\ud835\udc51\u2032\u2032and\ud835\udc51\u2032\u2032is the hidden size of the Q-Former. Z\ud835\udc56is\nfurther projected to match the input space of the language model:\nE\ud835\udc56=W\ud835\udc59Z\ud835\udc56,W\ud835\udc59\u2208R\ud835\udc51\ud835\udc3f\ud835\udc40\u00d7\ud835\udc51\u2032\u2032(6)\nwhere E\ud835\udc56\u2208R\ud835\udc3e\u00d7\ud835\udc51\ud835\udc3f\ud835\udc40serves as input to the language model. At\ninference time, when the ground truth caption is not available,\nthe Feature-Text Alignment module relies solely on the projected\nimage features F\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc57\n\ud835\udc56to generate the enriched embeddings Z\ud835\udc56. These\nembeddings are subsequently forwarded to the caption generation\nmodule, completing the reconstruction pipeline.\n3.1.3 Caption Generation By using these outputs from the Feature-\nText Alignment Module, CapRecover further employs a large lan-\nguage model (LLM) to interpret the compressed image representa-\ntions and generate the final caption that accurately describes the\nimage\u2019s semantic content. Specifically, the LLM processes the input\nembeddings E\ud835\udc56and, if provided, extra text input embeddings T\ud835\udc56\n(e.g., prompts), to generate the caption \ud835\udc36\ud835\udc56for the image \ud835\udc3c\ud835\udc56. This\ncaption generation process is modeled autoregressively as follows:\n\ud835\udc43(\ud835\udc36\ud835\udc56|E\ud835\udc56,T\ud835\udc56)=\ud835\udc47\u00d6\n\ud835\udc61=1\ud835\udc43(\ud835\udc50\ud835\udc56,\ud835\udc61|\ud835\udc50\ud835\udc56,<\ud835\udc61,E\ud835\udc56,T\ud835\udc56), (7)\nwhere\ud835\udc50\ud835\udc56,\ud835\udc61denotes the token generated at time step \ud835\udc61and\ud835\udc50\ud835\udc56,<\ud835\udc61\nrepresents all preceding tokens before time step \ud835\udc61.\n3.2 Model Training Objective and Settings\nTo train the model, we minimize the cross-entropy loss between\nthe generated caption \ud835\udc36\ud835\udc56and the ground-truth caption \ud835\udc36\u2217\n\ud835\udc56:\nL=\u22121\n\ud835\udc41\ud835\udc41\u2211\ufe01\n\ud835\udc56=1\ud835\udc47\u2211\ufe01\n\ud835\udc61=1log\ud835\udc43(\ud835\udc50\u2217\n\ud835\udc56,\ud835\udc61|\ud835\udc50\u2217\n\ud835\udc56,<\ud835\udc61,E\ud835\udc56,T\ud835\udc56) (8)\nwhere\ud835\udc41is the batch size, \ud835\udc47is the caption length, and \ud835\udc50\u2217\n\ud835\udc56,\ud835\udc61is the\nground-truth.The feature projection module in CapRecover is initialized with\na random distribution, while employing a pre-trained Q-Former\nmodel for the feature-text alignment and a pre-trained OPT model\nfor language generation. To focus the training on aligning the visual\nfeatures with the corresponding textual information, we freeze the\nparameters of the language model and update only those in the\nfeature projection and feature-text alignment modules.\nCapRecover is trained for six epochs with a learning rate of\n5\ud835\udc52\u22125. The training batch size is configured to 16, while the testing\nbatch size is set at 8. All experiments are conducted on a cloud\nserver equipped with a single NVIDIA RTX 4090 (24 GB memory).\n4 Experiments on Caption Reconstruction\n4.1 Experimental Settings\n4.1.1 Datasets To comprehensively evaluate the effectiveness of\nCapRecover , we adopt three widely-used datasets: COCO2017 [ 15],\nFlickr8K [ 8], and ImageNet-1K [ 2]. For the ImageNet-1K dataset,\nwe use Qwen2.5 [ 30] to generate captions for the images. We em-\nploy generated captions for ImageNet-1K due to: (1) the original\nImageNet-1K dataset does not provide human-annotated captions,\nand (2) recent research [ 11,20] demonstrates the effectiveness and\nsemantic accuracy of captions generated by advanced VLMs. We\nwill clarify this in the revised version. Given the large size of the\noriginal ImageNet-1K dataset, we randomly sample 12,000 images\nfor the training set and 1,000 images for the test set. More details\nabout these datasets are provided in Table 1.\n4.1.2 Victim Models We focus on three widely adopted visual mod-\nels commonly utilized in Vision-Language Models (VLMs) and de-\nployed on edge devices: Vision Transformer [ 3] (ViT), ResNet [ 5],\nand MobileNet (MobileNetV2 [ 23] and MobileNetV3 [ 9]). In prac-\ntice, ResNet (e.g., ResNet50 and ResNet101) serves as the image\nencoder in VLMs like CLIP [ 22] and UPL [ 10]. ViT (e.g., ViT-16B\nand ViT-32B) serves as the visual module in VLMs such as CLIP\n[22] and LlaVa [ 16]. As a lightweight convolutional neural network\noptimized for mobile applications, MobileNet is widely deployed\non edge devices like mobile phones, offering efficient performance\nfor on-device inference.\nWe analyze both the final output of the victim model (referred to\nas the \u201cbase\u201d output) and the intermediate output before the final\nlinear projection layer (denoted as \u201cno-proj\u201d for ViT-based models\nand \u201clayer4\u201d for ResNet-based models). Additionally, we examine\nthe impact of different middle layers within the victim models (e.g.,\n\u201clayer1\u201d \u223c\u201clayer4\u201d in ResNet50) on reconstruction performance.\n4.1.3 Evaluation Metrics We evaluate CapRecover \u2019s performance\nusing two main categories of metrics: standard metrics and semantic\nsimilarity metrics based on cosine similarity.\nCommon Metrics: We adopt widely used evaluation measures, in-\ncluding: BLEU-1 \u223cBLEU-4, METEOR, ROUGE_L, CIDEr, and SPICE,\nto assess the quality of the generated captions. These metrics quan-\ntify how closely the generated captions match the ground truth\ncaptions in terms of lexical overlap. We primarily rely on ROUGE-L\nas our main metric, since it captures structural alignment and se-\nmantic completeness more effectively than n-gram-based scores.\nWe consider ROUGE-L scores above 0.3 as indicative of moderate at-\ntack success, reflecting partial semantic recovery, and scores above\n\n--- Page 5 ---\nCapRecover : A Cross-Modality Feature Inversion Attack Framework on Vision Language Models MM \u201925, October 27\u201331, 2025, Dublin, Ireland.\nTable 3: Experimental results of CapRecover attacking different victim models on three datasets.\nDataset Victim model\u2217BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr SPICE Cosine Similarity (%)\u2021\nCOCO2017CLIP ViT16 0.72 0.55 0.41 0.30 0.26 0.53 0.99 0.19 84.38\nCLIP ViT32 0.70 0.53 0.39 0.29 0.26 0.53 0.95 0.19 80.38\nRN50 0.70 0.52 0.38 0.28 0.25 0.52 0.90 0.18 76.84\nRN101 0.70 0.52 0.39 0.28 0.25 0.53 0.93 0.18 79.98\nMNV2 0.39 0.18 0.10 0.06 0.11 0.31 0.09 0.03 0.44\nMNV3 0.40 0.19 0.10 0.08 0.11 0.31 0.10 0.03 2.74\nFlickr8KCLIP ViT16 0.30 0.15 0.08 0.05 0.13 0.27 0.54 0.19 22.40\nCLIP ViT32 0.29 0.15 0.08 0.05 0.12 0.26 0.48 0.17 18.40\nRN50 0.28 0.14 0.08 0.04 0.12 0.25 0.46 0.17 16.50\nRN101 0.28 0.14 0.08 0.05 0.12 0.25 0.47 0.17 18.00\nMNV2 0.20 0.08 0.04 0.02 0.07 0.17 0.16 0.06 1.20\nMNV3 0.21 0.08 0.04 0.02 0.07 0.17 0.17 0.06 1.80\nImageNet-1KCLIP ViT16 0.45 0.30 0.21 0.15 0.18 0.41 1.18 0.2 40.78\nCLIP ViT32 0.44 0.29 0.20 0.14 0.18 0.40 1.08 0.19 36.11\nRN50 0.42 0.27 0.18 0.13 0.16 0.38 0.93 0.16 27.00\nRN101 0.42 0.27 0.18 0.13 0.16 0.38 0.95 0.16 27.78\nMNV2 0.29 0.15 0.07 0.03 0.09 0.26 0.18 0.03 1.67\nMNV3 0.29 0.14 0.07 0.04 0.09 0.26 0.22 0.04 6.11\n\u2217\u201cRN50\u201d (\u201cRN101\u201d) denotes ResNet50 (ResNet101) and \u201cMNV2\u201d (MNV3) denotes MobileNetV2 (MobileNetV3).\n\u2021We calculate the proportion of cosine similarities that are greater than a predefined threshold, which is empirically set to 0.7 in our paper.\n0.0 0.2 0.4 0.6 0.8 1.0\nCosine Similarity0%10%20%30%40%50%PercentageViT-16B\nViT-32B\nResNet50\nResNet101\nMobileNetV2\nMobileNetV3\n(a) Evaluation on COCO2017 dataset.\n0.0 0.2 0.4 0.6 0.8 1.0\nCosine Similarity0%10%20%30%40%50%PercentageViT-16B\nViT-32B\nResNet50\nResNet101\nMobileNetV2\nMobileNetV3 (b) Evaluation on Flickr8K dataset.\n0.0 0.2 0.4 0.6 0.8 1.0\nCosine Similarity0%10%20%30%40%50%PercentageViT-16B\nViT-32B\nResNet50\nResNet101\nMobileNetV2\nMobileNetV3 (c) Evaluation on ImageNet-1K dataset.\nFigure 3: Distribution of cosine similarities across three datasets. We use intermediate features extracted from the final layer of\nthe victim model to train CapRecover . We analyze how other intermediate layers\u2019 features impact performance in Sec. 4.3.\n0.5 as indicative of successful attacks that capture most of the key\nsemantic content.\nEmbedding-Based Cosine Similarity: In addition to the common\nmetrics, we use a pre-trained embedding model [ 30] to project both\nthe generated and ground truth captions into a shared semantic\nspace. We then compute the cosine similarity between these em-\nbeddings to measure the semantic alignment between the captions.\nWe interpret similarity values above 0.7 as successful attacks.\nNote that as far as we know, our work mainly focuses on the\ndirect recovery of image captions or labels from leaked intermediate\nfeatures rather than reconstructing images first. To our knowledge,\nno prior studies address this specific problem.\n4.2 Experimental Results\n4.2.1 Overall results We evaluate the performance of CapRecover\non six victim models across three benchmark datasets: COCO2017,\nFlickr8K, and ImageNet-1K. As shown in Table 3, CapRecover\nachieves the strongest results on the COCO2017 dataset. For exam-\nple, when attacking CLIP ViT16 , the model achieves a BLEU-1 score\nof 0.72 and a ROUGE-L score of 0.53, with 84.38% of generated\ncaptions exceeding a cosine similarity threshold of 0.7\u2014indicating\nstrong semantic and structural alignment with the ground truth.\nSimilar performance is observed for other ViT- and ResNet-based\nvictim models, with ROUGE-L scores consistently around 0.52\u20130.53,\nwhich we consider indicative of successful semantic inversion.In contrast, performance on the Flickr8K dataset is substantially\nlower across all models. For instance, CLIP ViT16 yields a BLEU-1\nscore of only 0.30 and a ROUGE-L of 0.27, and the proportion of\ncosine similarities exceeding 0.7 drops to 22.40%. This degradation\nis largely due to the small size of Flickr8K (8,000 images) and the\nnature of its captions, which are shorter, less diverse, and often\nsemantically sparse. Such properties limit the model\u2019s ability to\nlearn rich visual-to-text mappings and result in lower alignment\non both lexical and structural metrics.\nResults on ImageNet-1K fall between the two extremes. Despite\nbeing a classification dataset without explicit captions, ViT- and\nResNet-based models still enable moderate recovery: CLIP ViT16\nyields a BLEU-1 of 0.45, ROUGE-L of 0.41, and cosine similarity over\n40%. These results suggest that classification-pretrained encoders\nimplicitly retain a significant amount of caption-relevant semantic\ninformation in their intermediate features.\nAcross all three datasets, MobileNetV2 and MobileNetV3 consis-\ntently show the weakest performance. For example, on COCO2017,\ntheir ROUGE-L scores are only 0.31 and their cosine similarities\nbarely exceed 3%, indicating poor semantic preservation. We at-\ntribute this to the lightweight, efficiency-oriented design of Mo-\nbileNet models. MobileNet utilizes depthwise separable convolu-\ntions and aggressive dimensionality reduction strategies designed\nfor efficiency, which reduce model complexity but significantly\ncompromise the model\u2019s ability to capture detailed and high-level\nsemantic features [ 13,14,26]. This architectural limitation inher-\nently leads to less detailed intermediate features, explaining the\n\n--- Page 6 ---\nMM \u201925, October 27\u201331, 2025, Dublin, Ireland. Xiu et al.\nOriginal Image\n\u201cA man on his skis on a snowy slope.\u201d\nRN50 -Layer1\n\u201cI am going to take a look at the world of \n(repetitive words)\u2026\u201d\nRN50 -Layer2\n\u201cA large group of skiers are in the snow.\u201d\nRN50 -Layer3\n\u201cA group of people skiing down a \nmountain in the snow.\u201d\nRN50 -Layer4\n\u201cA man riding two skis across a snowy field.\u201d\nNormalized Activation\nFigure 4: Example of visualizing the heatmaps of ResNet50\u2019s different middle layers. Below each figure is the generated/ground\ntruth caption. These figures demonstrate that the shallow layer (e.g., RN50-Layer1) pays more attention to edges and local\nfeatures, while the deeper the layer (e.g., RN50-Layer4), the more attention is paid to the more semantic areas in the image.\n0.0 0.2 0.4 0.6 0.8 1.0\nBLEU-1 score0%5%10%15%20%25%Percentage\n(a) ResNet50-layer1.\n0.0 0.2 0.4 0.6 0.8 1.0\nBLEU-1 score0%5%10%15%20%25%Percentage (b) ResNet50-layer2.\n0.0 0.2 0.4 0.6 0.8 1.0\nBLEU-1 score0%5%10%15%20%Percentage (c) ResNet50-layer3.\n0.0 0.2 0.4 0.6 0.8 1.0\nBLEU-1 score0%5%10%15%20%Percentage (d) ResNet50-layer4.\n0.0 0.2 0.4 0.6 0.8 1.0\nBLEU-1 score0%5%10%15%20%Percentage (e) ResNet50-base.\nFigure 5: Evaluate the distributions of cosine similarity on the COCO2017 dataset. We train CapRecover using the intermediate\nimage features produced by their final linear projection layers. We further discuss how other middle layers\u2019 intermediate\nfeatures affect CapRecover \u2019s performance in Sec. 4.3.\nlower effectiveness of CapRecover on MobileNet. We will clarify\nthis in the revised version. A more detailed analysis of layer-wise\nperformance is provided in Section 4.3.\n4.3 Further Study on Middle Layers\nAs shown in Table 4, we employ CLIP ViT16 and ResNet50 as victim\nmodels to investigate how intermediate image features from dif-\nferent layers impact caption reconstruction. Our analysis reveals\nthat features extracted from shallow layers contribute minimally\nto caption reconstruction because they primarily capture low-level\nvisual characteristics, such as edges and textures.\nIn contrast, the intermediate image features from deeper layers\nsignificantly enhance CapRecover \u2019s performance, as evidenced by\nhigher BLEU-1 scores compared to those obtained from shallow\nlayers. This finding suggests that as convolutional layers deepen,\nthey capture more specific and meaningful semantic information\nfrom the image. Figure 5 illustrates this trend by showing that the\nBLEU-1 score distribution for captions generated by CapRecover\non the COCO2017 dataset shifts to the right as layer depth increases,\nreflecting improved overall prediction accuracy.Furthermore, Figure 4 visualizes heat maps of different convolu-\ntional layers in ResNet50 for a target image. When CapRecover\nuses features from a shallow layer (e.g., ResNet50-layer1), which\ncaptures basic semantics such as the edges of a mountain or human,\nthe generated captions are less accurate and may even meaningless.\nHowever, as the intermediate features come from deeper layers,\nCapRecover gradually captures more relevant information from\nthe image (such as \u201csnow\u201d, \u201cskiing\u201d and \u201cman\u201d). When utilizing fea-\ntures from a deep middle layer (e.g., ResNet50-layer4), the generated\ncaption closely approximates the ground truth caption.\n5 Experiments on Label Recovery\nIn this section, we explore extending CapRecover to additional\nVision-Language Model (VLM) application scenarios\u2014specifically,\nimage label recovery . While our method is primarily introduced in\nthe context of image caption reconstruction, the underlying archi-\ntecture and attack strategy are general and easily transferable. To\nadapt CapRecover (as shown in Figure 2) for classification recon-\nstruction tasks, we replace the original large language model (LLM)\ncomponent with a standard linear classifier, as our preliminary\n\n--- Page 7 ---\nCapRecover : A Cross-Modality Feature Inversion Attack Framework on Vision Language Models MM \u201925, October 27\u201331, 2025, Dublin, Ireland.\nTable 4: Comparison of experimental results on ResNet50\nand CLIP ViT16 using different middle layers.\nVictim model Middle layer BLEU-1 CIDEr Cosine Similarity (%)\nResNet50layer1 0.24 0.19 0.00\nlayer2 0.51 0.31 43.76\nlayer3 0.58 0.55 31.42\nlayer4 0.62 0.68 85.64\nbase 0.70 0.90 90.52\nCLIP ViT16no-proj 0.69 0.90 93.04\nbase 0.72 0.99 94.76\n0.0 0.2 0.4 0.6 0.8 1.0\nCosine Similarity0%10%20%30%40%50%60%70%80%PercentageResNet50-layer1\nResNet50-layer2\nResNet50-layer3\nResNet50-layer4\nResNet50-base\nFigure 6: Embedding Cosine Similarity distributions for dif-\nferent middle layers of ResNet50. The results indicate that\nCapRecover employs the deep layers and may perform bet-\nter compared to the shallow layers.\nexperiments indicate that employing a simple classifier is enough\nto achieve high Top-1 accuracy.\n5.1 Experimental Settings\n5.1.1 Dataset As shown in Table 1, we evaluate CapRecover on\nthe CIFAR-10 and TinyImageNet datasets. CIFAR-10 consists of\n60,000 images evenly distributed across 10 classes, with 50,000 im-\nages used for training and 10,000 for testing. TinyImageNet contains\n200 classes with 500 training images and 50 validation images per\nclass (a total of 100,000 images). Due to the increased dataset com-\nplexity and class granularity, TinyImageNet serves as a challenging\nbenchmark for label recovery attacks.\n5.1.2 Victim models Similar to the setting in Sec. 4.1.2, the victim\nmodels we selected are ResNet-50 (i.e., RN50) and a CLIP variant\nwith a ViT-B/32 backbone (i.e., CLIP ViT16 ), from which intermediate\nvisual features are extracted as inputs to our CapRecover . We use\nthe final output of these victim models (i.e., \u201cbase\u201d output) as the\nintermediate features.\n5.1.3 Model settings We train the label recovery model using the\nAdam optimizer with a learning rate of 5\u00d710\u22124, a batch size of 64 for\ntraining and 16 for evaluation. The model is trained for 5 epochs in\ntotal. All experiments are conducted using standard cross-entropy\nloss for classification.\n5.2 Overall Experimental Results\nTable 5 shows the consistently strong performance of CapRecover\non image label recovery tasks across two widely-used datasets\n(CIFAR-10 and TinyImageNet) and two victim models (ResNet50\nand CLIP ViT32 ). Specifically, CapRecover achieves particularly high\nTop-1 and Top-5 accuracy scores on CIFAR-10 dataset. Notably,Table 5: Experimental results of image label recovery on\nCIFAR-10 and TinyImageNet datasets.\nVictim model datasets Top-1 Accuracy (%) Top-5 Accuracy (%)\nResNet50CIFAR-10 83.35 99.55\nTinyImageNet 60.13 83.79\nCLIP ViT32CIFAR-10 92.71 99.82\nTinyImageNet 72.62 91.60\nTable 6: Results of image label recovery on CIFAR-10.\nClass Precision Recall F1-Score\nAirplane 0.93 0.96 0.95\nAutomobile 0.96 0.97 0.97\nBird 0.91 0.90 0.91\nCat 0.85 0.85 0.85\nDeer 0.91 0.93 0.92\nDog 0.88 0.87 0.87\nFrog 0.92 0.94 0.93\nHorse 0.97 0.94 0.96\nShip 0.96 0.96 0.96\nTruck 0.97 0.96 0.96\nwhen attacking CLIP ViT32 ,CapRecover achieves a Top-1 accuracy\nof 92.71% and Top-5 accuracy of 99.82%, indicating near-perfect\nrecovery of class labels.\nWhile testing the TinyImageNet dataset, due to its larger number\nof classes and higher semantic complexity, CapRecover achieves\nlower accuracy scores compared to the results on CIFAR-10. How-\never, even under this challenging setting, CapRecover still achieves\n72.62% Top-1 accuracy and 91.60% Top-5 accuracy when targeting\nCLIP ViT32 , demonstrating the model\u2019s strong generalization abil-\nity across both datasets and victim architectures. In comparison,\nattacks on ResNet50 yield lower performance across both datasets,\nsuggesting that visual representations from CLIP-based encoders\nare more vulnerable to semantic leakage.\n5.3 Further Analysis on CIFAR-10\nGiven the large number of categories in the TinyImageNet dataset,\nwhich makes detailed per-class analysis less tractable, we focus\nour in-depth experimental analysis on the CIFAR-10 dataset. Based\non the experimental results presented in Figure 7 and Table 6, we\nobserve that CapRecover demonstrates strong performance in re-\nconstructing the classification labels of objects from intermediate\nimage features on the CIFAR10 test set. The confusion matrix (Fig-\nure 7) illustrates clear and distinct diagonal patterns, indicating\nthat predictions generally align closely with true labels. Most ob-\nject categories such as \u201cAutomobile\u201d, \u201cShip\u201d, and \u201cTruck\u201d achieve\nvery high correct prediction counts, approaching nearly perfect\nreconstruction accuracy.\nFurther quantitative analysis in Table 6 confirms these observa-\ntions. CapRecover achieves consistently high precision, recall, and\nF1-scores across all ten classes, with scores predominantly above\n0.90. The \u201cTruck\u201d and \u201cShip\u201d categories achieve particularly high\nscores (F1-scores of 0.96), demonstrating especially robust perfor-\nmance. Even categories with slightly lower performance, such as\n\u201cCat\u201d (F1-score of 0.85), remain sufficiently accurate to confirm the\nmodel\u2019s effectiveness.\nOverall, these experimental results indicate that CapRecover\neffectively reconstructs object classifications from intermediate\n\n--- Page 8 ---\nMM \u201925, October 27\u201331, 2025, Dublin, Ireland. Xiu et al.\nFigure 7: Confusion matrix of prediction results on CIFAR-10\ntest set. This matrix illustrates that CapRecover can accu-\nrately reconstruct the image classes.\nTable 7: Evaluation results of CapRecover attacking\nResNet50 with/without additional noise.\nDataset Middle layer layer1 layer2 layer3 layer4\nCOCO2017w/o noise 0.24 0.51 0.58 0.62\nw/ noise 0.49 0.03 0.02 0.05\nfeatures, highlighting its potential to successfully exploit feature\nleakage vulnerabilities in image recognition scenarios.\n6 Discussion on Potential Defense Mechanisms\n6.1 Noise-Based Feature Obfuscation\nTo protect intermediate representations in split DNN deployments,\nwe propose a lightweight noise-based defense mechanism that intro-\nduces random Gaussian noise into the intermediate features during\ninference. While this technique effectively degrades feature inver-\nsion attacks, its practical feasibility hinges on low communication\nand computation overhead, especially in edge\u2013cloud settings.\nLocal-only noise handling. To ensure deployment efficiency,\nour design ensures that both the injection and noise removal are\nperformed entirely on the client-side (i.e., the edge device). Specifi-\ncally, for any intermediate feature \ud835\udc39(\ud835\udc56), the edge device generates\na random noise vector \ud835\udf16(\ud835\udc56)\u223cN( 0,\ud835\udf0e2), computes the obfuscated\nrepresentation \u02dc\ud835\udc39(\ud835\udc56)=\ud835\udc39(\ud835\udc56)+\ud835\udf16(\ud835\udc56), and then removes the noise in the\nsubsequent layer before transmitting the result to the cloud:\n\ud835\udc39(\ud835\udc56+1)=\ud835\udc54(\u02dc\ud835\udc39(\ud835\udc56)\u2212\ud835\udf16(\ud835\udc56))=\ud835\udc54(\ud835\udc39(\ud835\udc56)).\nThe noise is neither stored nor transmitted\u2014thus, incurring no\nadditional communication cost and avoiding the need for synchro-\nnization with the cloud.\nNegligible computational overhead. The only extra compu-\ntation required is sampling from a standard Gaussian distribution\nand applying addition/subtraction operations\u2014both of which are\nlightweight and can be efficiently executed on modern edge hard-\nware (e.g., CPUs or NPUs). In our measurements, the time overheadintroduced per inference was negligible ( <1%relative increase),\nmaking this defense practical for real-time applications.\nSecurity benefit. By ensuring that the intermediate features\ntransmitted to the server are never raw (i.e., always processed),\nattackers who intercept these representations cannot reconstruct\naccurate semantic content without knowledge of the locally gener-\nated\ud835\udf16(\ud835\udc56). Moreover, since the noise is regenerated for each image,\neven partial leaks from one instance do not compromise others.\nOverall, his defense achieves a strong trade-off between privacy\nprotection and deployment practicality. It requires no retraining,\nwith no changes to final predictions. It can be readily integrated\ninto edge-side inference pipelines with minimal modification.\n6.2 Potential for Homomorphic Encryption\nHomomorphic Encryption (HE) [ 27] represents a promising cryp-\ntographic approach to mitigating privacy risks associated with\nVision-Language Models (VLMs). In typical VLM deployments, in-\ntermediate image features generated by the visual encoder are\nfrequently transmitted between client devices and remote servers,\ncreating opportunities for attackers to intercept and exploit these\nrepresentations to reconstruct sensitive textual information, such\nas image captions. By encrypting these intermediate features ho-\nmomorphically, HE enables operations to be conducted directly on\nencrypted data without revealing the underlying plaintext features,\nthereby significantly reducing the risk of privacy leakage.\nThe primary advantage of employing HE in VLM scenarios lies in\nits capability to ensure data confidentiality even when intermediate\nfeatures are intercepted during transmission or while temporarily\nstored. Attackers accessing encrypted features would find it com-\nputationally infeasible to derive meaningful information without\nthe appropriate decryption keys, effectively safeguarding sensitive\ntextual descriptions embedded within the features.\nThere are practical precedents demonstrating the feasibility of\nHE in protecting model parameters and gradients within federated\nlearning scenarios. For example, NVIDIA researchers successfully\nintegrated homomorphic encryption with XGBoost [ 29], employing\nCUDA acceleration to achieve efficient privacy-preserving feder-\nated learning. Such cases provide encouraging evidence that similar\nstrategies could protect privacy from intermediate features, thus\nsecuring textual information from feature inversion attacks.\n7 Conclusion\nIn this paper, we focus on the cross-modality feature inversion\nattack, proposing CapRecover , a generic framework that recon-\nstructs image captions and classification labels directly from leaked\nintermediate image features. By leveraging a feature projection\nmodule and a feature-to-text alignment mechanism, CapRecover\neffectively recovers semantic information\u2014even when using fea-\ntures from the final linear projection layer of the visual encoder.\nOur extensive experiments demonstrate CapRecover \u2019s effective-\nness across multiple datasets and models. Furthermore, we propose\nan effective protection approach without additional training costs,\nthereby efficiently preventing attackers from reconstructing sensi-\ntive image information.\n\n--- Page 9 ---\nCapRecover : A Cross-Modality Feature Inversion Attack Framework on Vision Language Models MM \u201925, October 27\u201331, 2025, Dublin, Ireland.\nReferences\n[1]Stability AI. [n. d.]. Activating humanity\u2019s potential through generative AI . https:\n//stability.ai/.\n[2]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-\nageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on\nComputer Vision and Pattern Recognition . 248\u2013255. https://doi.org/10.1109/CVPR.\n2009.5206848\n[3]Alexey Dosovitskiy. 2020. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[4]Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu\nWang, Sisi Duan, and Xiaoyun Wang. 2023. FigStep: Jailbreaking Large Vision-\nlanguage Models via Typographic Visual Prompts. arXiv:2311.05608 [cs.CR]\n[5]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770\u2013778.\n[6]Ying He, Jingcheng Fang, F Richard Yu, and Victor C Leung. 2024. Large language\nmodels (LLMs) inference offloading and resource allocation in cloud-edge com-\nputing: An active inference approach. IEEE Transactions on Mobile Computing\n(2024).\n[7]Zecheng He, Tianwei Zhang, and Ruby B Lee. 2019. Model inversion attacks\nagainst collaborative inference. In Proceedings of the 35th Annual Computer Secu-\nrity Applications Conference (ACSAC) . 148\u2013162.\n[8]Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image descrip-\ntion as a ranking task: Data, models and evaluation metrics. Journal of Artificial\nIntelligence Research (Aug. 2013), 853\u2013899. https://doi.org/10.1613/jair.3994\n[9]Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingx-\ning Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al .2019.\nSearching for mobilenetv3. In Proceedings of the IEEE/CVF international conference\non computer vision . 1314\u20131324.\n[10] Tony Huang, Jack Chu, and Fangyun Wei. 2022. Unsupervised prompt learning\nfor vision-language models. arXiv preprint arXiv:2204.03649 (2022).\n[11] Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, and Dacheng Tao. 2023. Image captions\nare natural prompts for text-to-image models. arXiv preprint arXiv:2307.08526\n(2023).\n[12] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language\nmodels. In International conference on machine learning . PMLR, 19730\u201319742.\n[13] Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang,\nSergey Tulyakov, and Jian Ren. 2023. Rethinking vision transformers for mo-\nbilenet size and speed. In Proceedings of the IEEE/CVF international conference on\ncomputer vision . 16889\u201316900.\n[14] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov,\nYanzhi Wang, and Jian Ren. 2022. Efficientformer: Vision transformers at mo-\nbilenet speed. Advances in Neural Information Processing Systems 35 (2022),\n12934\u201312949.\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 . Springer, 740\u2013\n755.\n[16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruc-\ntion Tuning.[17] Jinliang Lu, Ziliang Pang, Min Xiao, Yaochen Zhu, Rui Xia, and Jiajun Zhang.\n2024. Merge, ensemble, and cooperate! a survey on collaborative strategies in\nthe era of large language models. arXiv preprint arXiv:2407.06089 (2024).\n[18] Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. 2024.\nJailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large\nLanguage Models against Jailbreak Attacks. arXiv:2404.03027 [cs.CR]\n[19] Akrit Mudvari, Yuang Jiang, and Leandros Tassiulas. 2024. Splitllm: Collaborative\ninference of llms for model placement and throughput optimization. arXiv\npreprint arXiv:2410.10759 (2024).\n[20] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig\nSchmidt. 2023. Improving multimodal datasets with image captioning. Advances\nin neural information processing systems 36 (2023), 22047\u201322069.\n[21] OpenAI. 2024. Hello GPT-4o . https://openai.com/index/hello-gpt-4o/.\n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al.2021. Learning transferable visual models from natural language supervision.\nInInternational conference on machine learning . PMLR, 8748\u20138763.\n[23] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\nChieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In\nProceedings of the IEEE conference on computer vision and pattern recognition .\n4510\u20134520.\n[24] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2023. Plug and Pray:\nExploiting off-the-shelf components of Multi-Modal Models. arXiv preprint\narXiv:2307.14539 (2023).\n[25] Xinyue Shen, Yiting Qu, Michael Backes, and Yang Zhang. 2024. Prompt Stealing\nAttacks Against Text-to-Image Generation Models. In USENIX Security Sympo-\nsium (USENIX Security) . USENIX.\n[26] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag\nRanjan. 2023. Fastvit: A fast hybrid vision transformer using structural reparam-\neterization. In Proceedings of the IEEE/CVF international conference on computer\nvision . 5785\u20135795.\n[27] Wikipedia. [n. d.]. Homomorphic encryption . https://en.wikipedia.org/wiki/\nHomomorphic_encryption.\n[28] Xiaoyang Xu, Mengda Yang, Wenzhe Yi, Ziang Li, Juan Wang, Hongxin Hu,\nYong Zhuang, and Yaxin Liu. 2024. A Stealthy Wrongdoer: Feature-Oriented\nReconstruction Attack against Split Learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 12130\u201312139.\n[29] Ziyue Xu, Yuan-Ting Hsieh, Zhihong Zhang, Holger R Roth, Chester Chen,\nYan Cheng, and Andrew Feng. 2025. Secure Federated XGBoost with CUDA-\naccelerated Homomorphic Encryption via NVIDIA FLARE. arXiv preprint\narXiv:2504.03909 (2025).\n[30] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al .2024. Qwen2. 5\ntechnical report. arXiv preprint arXiv:2412.15115 (2024).\n[31] Mingjin Zhang, Xiaoming Shen, Jiannong Cao, Zeyang Cui, and Shan Jiang. 2024.\nEdgeshard: Efficient llm inference via collaborative edge computing. IEEE Internet\nof Things Journal (2024).\n[32] Xiaochen Zhu, Xinjian Luo, Yuncheng Wu, Yangfan Jiang, Xiaokui Xiao, and\nBeng Chin Ooi. 2025. Passive Inference Attacks on Split Learning via Adversarial\nRegularization. In Network and Distributed System Security (NDSS) Symposium\n2025.",
  "project_dir": "artifacts/projects/enhanced_cs.CV_2507.22828v1_CapRecover_A_Cross_Modality_Feature_Inversion_Att",
  "communication_dir": "artifacts/projects/enhanced_cs.CV_2507.22828v1_CapRecover_A_Cross_Modality_Feature_Inversion_Att/.agent_comm",
  "assigned_at": "2025-07-31T21:13:09.740031",
  "status": "assigned"
}